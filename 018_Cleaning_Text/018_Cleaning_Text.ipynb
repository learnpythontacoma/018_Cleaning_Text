{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='graphics/text_cleaning.png'>\n",
    "\n",
    "<img src='graphics/spacer.png'>\n",
    "\n",
    "<center><font style=\"font-size:40px;\">Cleaning Text from Web Scraping Indeed </font></center>\n",
    "<center>Prepared and coded by Ben P. Meredith, Ed.D.</center>\n",
    "\n",
    "\n",
    "When we were last together, we began developing a program to web scrape job announcements from Indeed. In our program, we saved vital information from the job announcements to a Pandas DataFrame. \n",
    "\n",
    "We were also left with a few tasks to code prior to today's discussion. As you may recall, I tasked you to do the following:\n",
    "\n",
    ">1. Find and remove duplicate job announcements\n",
    ">1. Identify if a table already exists for a search term\n",
    "    - if it does exist, add new entries to the bottom of the table\n",
    "    - find and remove duplicate job announcements\n",
    "\n",
    "If you took the opportunity to work on the code for this program, you realized that the second task (Identify if a table already exists for a search term) required you to do a bit of investigation on your own. I hope that you took advantage of this opportunity and went out to StackOverFlow.com for your research. This task was not one that we covered prior to your challenge, but we will cover it in this notebook. \n",
    "\n",
    "There was a third task that we needed to do in order to make our data more valuable. If you took the opportunity to look at the data as it was pulled from Indeed, you will have noticed that it is far from clean. In fact, it is downright dirty with HTML marks and code. Later in this notebook, we will work together on cleaning that data so it is easier for us to process and thus work with it. This work will be our introduction to the data science sub-field called **Natural Language Processing**, which we will discuss below. But first, let's go over the assigned tasks and look at solutions. \n",
    "\n",
    "# Loading Fresh Libraries and Data\n",
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T19:50:16.727068Z",
     "start_time": "2020-05-25T19:50:16.238015Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import our needed libraries\n",
    "\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Pull Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T19:50:18.298165Z",
     "start_time": "2020-05-25T19:50:18.291007Z"
    }
   },
   "outputs": [],
   "source": [
    "def job_data_pull(url):\n",
    "    page = requests.get(url)# go to the page noted by the url\n",
    "    page_contents = BeautifulSoup(page.content, 'lxml')#extract the contents of the page\n",
    "    \n",
    "    #only getting the tags for organic job postings and not the ones that are sponsored\n",
    "    tags = page_contents.find_all('div', {'data-tn-component' : \"organicJob\"})\n",
    "    \n",
    "    #getting the list of companies that have the organic job posting tags\n",
    "    companies = [x.span.text for x in tags]\n",
    "    \n",
    "    #extracting the features like the company name, complete link, date, etc.\n",
    "    attributes = [x.h2.a.attrs for x in tags]\n",
    "    dates = [x.find_all('span', {'class':'date'}) for x in tags]\n",
    "    \n",
    "    # update attributes dictionaries with company name and date posted\n",
    "    [attributes[i].update({'company': companies[i].strip()}) for i, x in enumerate(attributes)]\n",
    "    [attributes[i].update({'date posted': dates[i][0].text.strip()}) for i, x in enumerate(attributes)]\n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify if a Data Table with our Scrapes Already Exists using a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T19:50:19.780373Z",
     "start_time": "2020-05-25T19:50:19.776892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determine if a file exists within a pathway\n",
    "def find_file(pathway):#pathway is the path to the file\n",
    "    from pathlib import Path\n",
    "    pathway = Path(pathway)# convert the pathway to an actual path from a string\n",
    "    if pathway.exists():#Determine if the file exists\n",
    "        return 1 #1 = file exists\n",
    "    else: \n",
    "        return 0 # 0 = file does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T19:50:20.877224Z",
     "start_time": "2020-05-25T19:50:20.872160Z"
    }
   },
   "outputs": [],
   "source": [
    "#Initialize the data_log by discovering if it exists. If it does, load it. Otherwise, form one. \n",
    "def initialize_data_log(job_title):\n",
    "    import pandas as pd\n",
    "    job_title = job_title.replace(' ', '_')\n",
    "    data_file = ('data/'+job_title+'_job_search.csv')\n",
    "    answer = find_file(data_file)\n",
    "    if answer == 1:\n",
    "        df = pd.read_csv(data_file, index_col=0)\n",
    "        df = df.drop(['level_0'], axis=1, errors='ignore')#Drops level_0 column that keeps showing up \n",
    "    else:\n",
    "        df = pd.DataFrame(columns=('job_id', 'title', 'company', 'url', 'text', 'pull_date'))\n",
    "        df.to_csv(data_file)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Basic Scraping Program so We Can Grab Some Data for this Notebook Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T19:50:37.908770Z",
     "start_time": "2020-05-25T19:50:23.678515Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ask the user what job description they are interested in searching for and where\n",
    "job_title = input('What job description are you interested in searching? ')\n",
    "location = input('What is your zip code?' )\n",
    "\n",
    "#establish a DF to store the data if one does not exist\n",
    "#if a search term data already exists, use it.\n",
    "\n",
    "df = initialize_data_log(job_title)\n",
    "\n",
    "\n",
    "#Establishes the variables we will need for the Indeed Search URL\n",
    "getVars = {'q' : job_title, 'l' : location, 'fromage' : 'last', 'sort' : 'date'}\n",
    "\n",
    "#Assembles the Indeed Search URL from the attributes above\n",
    "url = ('https://www.indeed.com/jobs?' + urllib.parse.urlencode(getVars))\n",
    "\n",
    "#Using our uniquely assembled URL, we run the subroutine to get the job data from the function we defined above.\n",
    "answer = job_data_pull(url)\n",
    "\n",
    "starting_length = len(df)\n",
    "\n",
    "# Using the data gathered by our function, we are assigning information to our DataFrame AND we are \n",
    "# getting the information we need to retrieve the job description text. \n",
    "for index, a in tqdm(enumerate(answer)):\n",
    "    df.loc[starting_length + index, 'url'] = a['id'].replace('jl_', 'https://www.indeed.com/viewjob?jk=')\n",
    "    df.loc[starting_length + index, 'title'] = a['title']\n",
    "    df.loc[starting_length + index, 'company'] = a['company']\n",
    "    df.loc[starting_length + index, 'job_id'] = a['id']\n",
    "    df.loc[starting_length + index, 'pull_date'] = datetime.date(datetime.now())\n",
    "\n",
    "# Using the URLs we generated in line 20, we use them in this FOR loop to retrieve the job description text. \n",
    "for index, url in tqdm(enumerate(df.url)):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        content = BeautifulSoup(page.content, \"html.parser\")\n",
    "        job_text = content.find('div', class_=\"jobsearch-jobDescriptionText\")\n",
    "        df.loc[index, 'text'] = str(job_text)\n",
    "    except KeyError:\n",
    "        df.loc[index, 'text'] = str(job_text)\n",
    "\n",
    "job_title = job_title.replace(' ', '_')\n",
    "df.to_csv('data/'+job_title+'_job_search.csv')\n",
    "        \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T19:51:05.601919Z",
     "start_time": "2020-05-25T19:51:05.598664Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The .replace( ) Command\n",
    "\n",
    "When it comes to cleaning text, the most basic and most often used command is the `.replace()` command. As you can safely assume, the `.replace()` command replaces text or special characters (user defined) with a user defined word, letters, or special character. \n",
    "\n",
    "The syntax of the `.replace()` command is simple:\n",
    "\n",
    "> text = text.replace('what you want to replace', 'what you want it replaced with')\n",
    "\n",
    "Let's look at the following example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T19:51:08.095498Z",
     "start_time": "2020-05-25T19:51:08.092898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our test string\n",
    "\n",
    "test_text = 'The jolly merchant of Verona'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our `test_text`, we want to replace the word 'jolly' with the word 'happy'. Below is how we do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T19:51:09.626262Z",
     "start_time": "2020-05-25T19:51:09.622832Z"
    }
   },
   "outputs": [],
   "source": [
    "test_text = test_text.replace('jolly', 'happy')\n",
    "\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first example, we replaced an entire word. In the next example, we want to replace the punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T19:56:56.699321Z",
     "start_time": "2020-05-25T19:56:56.691216Z"
    }
   },
   "outputs": [],
   "source": [
    "test_text = 'You are kidding.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T19:56:57.288966Z",
     "start_time": "2020-05-25T19:56:57.284617Z"
    }
   },
   "outputs": [],
   "source": [
    "test_text = test_text.replace('.', '!')\n",
    "\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Using the .replace( ) to clean text\n",
    "\n",
    "Now that we have covered the `.replace()` command, you can use it to develop a text cleaning function. \n",
    "\n",
    ">1. In the next block, pull the text from several rows in your DataFrame to examine them\n",
    ">1. Develop a text cleaning function using the `.replace()` command\n",
    ">1. When you have a function that works for you, run every entry in the ['text'] column in the DataFrame through the function and save the results in a new column titled 'clean_text'.\n",
    ">1. Finally, print out a sample of 5 entries from the 'clean_text' column alone to make sure your function is working. \n",
    "\n",
    "<font color='red'>**HINT**:</font> Think carefully about what you pull and what you replace text with. If you consider it carefully rather than jumping right into the task, you can make your text human readable and human pretty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:01:26.875971Z",
     "start_time": "2020-05-25T20:01:26.869548Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write a FOR loop that prints out the text from the first four rows of our DataFrame. \n",
    "# Make a separator between each full text entry - try \"print('\\n','*'*72)\" to see what this does. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:05:32.869868Z",
     "start_time": "2020-05-25T20:05:32.863819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write a function using the .replace() command that cleans the text. \n",
    "# Hint: It may take several lines of code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:05:33.474170Z",
     "start_time": "2020-05-25T20:05:33.469093Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write a FOR loop that cleans and prints out the text from the first four rows of our DataFrame.\n",
    "# Place a separator between the text. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write a loop that \n",
    ">1. pulls the raw text from each row in the DataFrame,\n",
    ">1. cleans the raw text,\n",
    ">1. places the clean text in a column called 'clean_text' in the appropriate row for each text, and\n",
    ">1. print out a sample of five entries from the DataFrame to verify our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:02:32.047312Z",
     "start_time": "2020-05-22T15:02:31.987922Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Words \n",
    "\n",
    "\"Tokenizing\" is the splitting of a text entry into smaller parts, which we call 'tokens'. Tokens can be individual words, individual sentences, or individual paragraphs depending upon what unit of measure you want to use. For our purposes right now, we want to tokenize our clean_text by words.\n",
    "\n",
    "There are several techniques to splitting up a string into words. We will look at two methods. \n",
    "\n",
    "### Method 1: The .split( ) Command\n",
    "\n",
    "The `.split()` command is not specific to tokenizing words, but we can use it to split a string into words. The `.split()` command can be used to split a string almost anyway that you want it to split; how it splits a string depends upon how you want it to split. \n",
    "\n",
    "The syntax of the `.split()` command is simple:\n",
    "> text = text.split('what spaces, letters or character by which you want the string split')\n",
    "\n",
    "In the function below, notice this syntax in line 3. \n",
    "\n",
    "But what are we doing in line 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:02:35.947781Z",
     "start_time": "2020-05-22T15:02:35.944339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to using the split command to tokenize a string by words. \n",
    "\n",
    "def tokenize_by_words2(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.split(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using nltk to Tokenize by Words\n",
    "\n",
    "The second method to tokenize a string by words is to use the nltk library's `word_tokenize()` function. \n",
    "\n",
    "\"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, and wrappers for industrial-strength NLP libraries.\" [https://www.nltk.org/]\n",
    "\n",
    "In the next function, you will note that\n",
    ">1. in line 4, I added an `import` statement import the needed function out of the nltk library\n",
    ">1. in lines 5 & 6, I added the `str()` command to the tokenized text so that it saves as a tokenized string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:02:37.073352Z",
     "start_time": "2020-05-22T15:02:37.070339Z"
    }
   },
   "outputs": [],
   "source": [
    "#Tokenize a document's text given as a string to the word level and returning a list of tokenized words\n",
    "\n",
    "def tokenize_by_words(text):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    token_text = str(word_tokenize(text))\n",
    "    return str(token_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning White Space\n",
    "\n",
    "\n",
    "### Clean Leading and Trailing White Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:45:56.582613Z",
     "start_time": "2020-05-25T20:45:56.579994Z"
    }
   },
   "outputs": [],
   "source": [
    "test_string = ' The   Red fox   Jumped  over The  Lazy dog. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:45:57.576850Z",
     "start_time": "2020-05-25T20:45:57.573430Z"
    }
   },
   "outputs": [],
   "source": [
    "phrase = test_string.strip()\n",
    "    \n",
    "print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean White Space using .replace( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:45:58.763277Z",
     "start_time": "2020-05-25T20:45:58.758706Z"
    }
   },
   "outputs": [],
   "source": [
    "test_string = test_string.replace('   ', ' ')\n",
    "test_string = test_string.replace('  ', ' ')\n",
    "test_string.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Text\n",
    "\n",
    "### Make all words lower case using .lower( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:02:43.434227Z",
     "start_time": "2020-05-22T15:02:43.430544Z"
    }
   },
   "outputs": [],
   "source": [
    "test_string.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all words upper case using .upper( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:02:44.337696Z",
     "start_time": "2020-05-22T15:02:44.333640Z"
    }
   },
   "outputs": [],
   "source": [
    "test_string.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the First Letter in Each Word Uppercase using .title( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:02:45.174031Z",
     "start_time": "2020-05-22T15:02:45.170414Z"
    }
   },
   "outputs": [],
   "source": [
    "test_string.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "As you look at the output from the line above, you see that we have spaces and at the end of the line. Each word in the string is capitalized. We want to correct these two issues. \n",
    "\n",
    "In the next block, using what you have learned in all of our lessons, write a function that would take any string and return a string that is correct with capitalization and non-capitalized words.\n",
    "\n",
    "After writing the function, run the function on the list of strings in the second block. Did you get it correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:48:54.399805Z",
     "start_time": "2020-05-25T20:48:54.396076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write a function to correct the text capitalization\n",
    "# The function should capitalize the first letter only in each string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:48:55.183028Z",
     "start_time": "2020-05-25T20:48:55.179508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the following strings list to test your function\n",
    "\n",
    "strings_list = ['THE large Box cOntained Her gift.', 'Jack DoeS Not LiKe Getting Water.']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now save our DataFrame for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:06:24.100744Z",
     "start_time": "2020-05-22T15:06:24.084766Z"
    }
   },
   "outputs": [],
   "source": [
    "job_title = job_title.replace(' ', '_')\n",
    "df.to_csv('data/'+job_title+'_job_search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
